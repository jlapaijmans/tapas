<span class="title">The TAPAS manual</span>

Dependencies of the scripts
===========================

The scripts consisting this software depend on 

  * A Python 3 installation
  * R

Furthermore, the scripts depend on several packages for R and Python.

To check whether all dependencies are met, execute the following
command. It is included in the `scripts/` folder.

```
scripts/setup_check_dependencies
```

Check the output of this script to find package names which are not
installed or can't be loaded because of eventual errors.

Then install these packages manually by using 

Install packages for R
----------------------

Install missing packages in R by executing the following command
inside the R command prompt (type `R` into your shell to invoke it)

```{.r}
install.packages("PACKAGENAME")
```

Install packages for Python
---------------------------

Invoke the Python package manager `pip3` to install packages for
Python:

```
pip3 install PACKAGENAME...
```

You may specify multiple package names by writing them one after
another, separated by whitespace.



Genome Preparation
==================

 
Record index table
------------------

First, index the genome using `samtools`. This is needed for several
downstream tools:
```{.sh}
samtools faidx data/genome/volpertinger.fasta
```



Generating sample reads
=======================

The output FASTQ file will be constructed by interleaving the contents
of three files, containing the read names, the nucleotide strings and
the quality strings, respectively. These files will be generated in
the following. 

Additionally, a table containing the read names with the true read
positions will be created, to evaluate later on whether a read was
mapped correctly.

Generate nucleotide strings and read names
------------------------------------------

The `uniform` script can be used to sample reads from a reference
genome. The script needs a FASTA file as input, as well as the desired
amount of reads to be generated, the minimum length and the decay
length. The read lengths are exponentially distributed; the decay
length parameter specifies the length by which half of the reads are
longer than the minimum read length.

For the FASTA file, an index file must exist which was generated in
the previous section using samtools.

Example: Extract 25 reads, with minimum length 20 basepairs, where
half of the reads have a length longer than 25 bp. The value 123 
is used to initialize the random number generator, i.e. can be used to
obtain reproducible reads. This last parameter can be omitted.

The resulting output will be a raw list of nucleotide sequences and a
table containing three columns:

 1. The FASTA record name (e.g. chromosome) where the read originated.
 2. 1-based base number of the reads' first base.
 3. 1-based base number of the reads' last base.


The base indices are 1-based because base indices are 1-based in
SAM-files as well. (whereas in BAM-files, they are 0-based, but we
don't need to deal with BAM-files here.)

We will generate sample reads from our sample genome of a
volpertinger.

Execute the following script to generate random nucleotide sequences:

```{.sh}
scripts/uniform data/genome/volpertinger.fasta \
    --seed 1234 \
    --output data/2/volpertinger.coord data/2/volpertinger.nucl \
    25 20 5 
```

Two files are generated when the `--output` switch is used, as is the
case above: One holds the read names and coordinates and the other one 
holds the raw nucleotide sequences. When omitting ``--output``, all
information is printed in tabular form on the standard output and not
saved to distinct files.

The resulting files look like this:

```{.sh}
head data/2/volpertinger.coord | column -t
```

Putting together the FASTQ file
-------------------------------

This task needs three input files: 

  1. The list of read names
  2. The list of nucleotide strings 
  3. The list of quality strings

The first list is extracted from the file `volpertinger.coord`, the second
list exists already (`volpertinger.nucl`) and the third list is generated
using standard UNIX tools from the nucleotide strings

Read names
----------

In this tutorial we generate read names consisting of the organism
name (*volpertinger*) followed by an underscore and a counting number.

To have the origin information of the reads available along with their
newly-generated reads, it is advisable to add the read names to the
`volpertinger.coord` file generated above.

There is a script included for adding this kind of column, which is
shown in the next code example. You can as well use `awk` or whichever
tool you like to accomplish this task if you need more sophisticated
read names. 

```{.sh}
scripts/index_column  --prefix volpertinger_ \
                      --colname name  \
                      --inplace data/2/volpertinger.coord

head data/2/volpertinger.coord | column -t
```

To use the read names to generate a FASTQ file, they must be available
as a raw list without additional columns or a header. `awk` can be
used to perform this task:

```{.sh}
awk '(NR!=1){print $1}' \
      data/2/volpertinger.coord \
    > data/2/volpertinger.i

head data/2/volpertinger.i
```

Quality strings
---------------

Currently, the effect of quality strings on the mapping result has not
been investigated. Currently only strings of constant quality score
are used. This can be done by the UNIX `sed` tool, which replaces every 
character by an F:

```{.sh}
sed 's/./F/g' \
      data/2/volpertinger.nucl \
    > data/2/volpertinger.q

head data/2/volpertinger.q
```

If you want to generate more elaborate quality strings, you are free
to do so using whichever tools you desire. Just generate a list with as
many lines as there are nucleotide strings in `volpertinger.nucl` to input them 
into the pipeline.

Putting the FASTQ file together
-------------------------------

The `synth_fastq` tool creates a FASTQ file from its components,
nucleotide string, quality string and read name (ID line). If the file
containing the read lines is omitted, the reads are numbered
sequentially.

```{.sh}
scripts/synth_fastq data/2/volpertinger.nucl \
                    data/2/volpertinger.q    \
                    data/2/volpertinger.i    \
    > data/2/volpertinger.fastq

head data/2/volpertinger.fastq
```

Repeat the above steps to generate contaminant reads
----------------------------------------------------

This commands generate some reads from a truncated *Rhizobium etli*
genome, to supply reads which are not supposed to map. This way,
contamination with non-endogenous reads are simulated.

The endogenous (`volpertinger.fastq`) and contaminant (`retli.fastq`) 
read will be merged into one fastq file (`all.fastq`) once the
sample reads have undergone mutation simulation. This will be 
done in the next section.

Note that two kinds of abbreviations are used here:

 *  Terms like `const_{a,b,c}` are expanded by `bash` to `const_a
    const_b const_c` and can therefore be used to specify mulitple
    paths which share some parts. 
 *  The temporary files (similar to `volpertinger.q` and `volpertinger.i` 
    above) are omitted here by using `bash`'s *process substitution*
    (`<(...)`) which uses the output of one argument instead of a file
    name which the other command expects.

If you do not understand why these commands are equivalent to the
commands listed above used to generate `reads.fastq`, you can use
these as well without problems.

Index genome and sample nucleotide seqences:
```{.sh}
samtools faidx data/retli/retli_tr.fasta

scripts/uniform \
    data/retli/retli_tr.fasta \
    --seed 2345 \
    --output data/2/retli.{coord,nucl} \
    25 20 5 
```

Read names:

```{.sh}
scripts/index_column  --prefix retli_ \
                      --colname name  \
                      --inplace data/2/retli.coord
```

Put the FASTQ together:

 * Quality strings are generated without an intermediate file using 
   `sed`
 * Read names are extracted without an intermediate file using `awk`


```{.sh}
scripts/synth_fastq \
    data/2/retli.nucl \
    <(sed 's/./F/g'           data/2/retli.nucl) \
    <(awk '(NR!=1){print $1}' data/2/retli.coord) \
    > data/2/retli.fastq

head data/2/retli.fastq
```



Mutation of reads
=================

Mutation probabilities
----------------------

The reads are mutated using per base probabilities derived from the
geometric distribution. The mutation probability at the read ends is
the highest. By this, the chemical damage near read ends can be
modelled. For this, three parameters are important:

  * The base-independent mutation probability $t$ ($0<t<1$). This is
    the probability of any base to mutate, regardless of its proximity
    to the end of the read. This can be used to model evolutionary
    distance.

  * The steepness $p$ ($0<p<1$) of the mutation probability decline
    when moving away from the read end. The higher this parameter the
    steeper is the decline of mutation probability when moving away
    from the read end.

  * The multiplying factor $f$. At the read end, $f+t$ is the
    probability of the first base of the read to be mutated.

With this model, its possible to archive mutation probabilities
greater than one. This makes of course no sense and the mutation
probability is cut back to one in such cases.

In mathematical notation, the mutation probability $P_{mut}$ of a base
number $x$, starting to count at the reads' end, is:

$$P_{mut}(x) = f \cdot dgeom(x;p) + t$$

with $dgeom(X;P)$ being the density function of the geometric
distribution, with parameters X = number of tries; P = success probability.

The following sketch graphs illustrate the three parameters:

<img src="fig/mut.svg" width="800" />

Specifying the parameters
-------------------------

The mutation probabilities are saved as a text table. It looks like
this:

```{.sh}
cat data/mut-tables/mut.tab
```

The columns have the following meaning:
 
  * strand:  [3 or 5] On which end of the read should base 1 be
    located, the base with the highest mutation probability
  * from: [Letter or *] Which bases should mutate according to this
    lines' parameters. * means this rule applies to every base.
  * to:   [Letter or *] Which base should a mutation event yield. 
          * means a base (A,T,C,G) different from the original base
          is chosen at random.
  * factor: $f$ from the previous section
  * geom_prob: $p$ from the previous section
  * intercept: $t$ from the previous section

In the example above, a base is $x$ bp away from the 5' end of
a read of length l. That means, it is $(l-x)$ bp away from the 3' end
of the read. The exchange probability depends on the type of the
nucleotide and on $x$:

Base at position $x$ is not Cytosine:

$$P_{\ast\rightarrow\ast}(x) = 0 \times dgeom(x;0.1) + 0.12 = 0.12 = 
P_{\ast\rightarrow\ast}$$

Base at position $x$ is Cytosine (C): 

$$P_{C\rightarrow T}(x) = P_{\ast\rightarrow\ast} + 
          (0.3\times dgeom(x;0.4) + 0.1) + 
          (0.1\times dgeom((l-x);0.2) + 0.0) $$

Mutate a sequence
-----------------

The `multiple-mutate.py` tool takes a table of the previous section as
input and mutates strings provided to it on standard input
accordingly. 

Mutations can be inserted either before a FASTQ file is assembled
(`synth_fastq.py`, see previous section), or afterwards. This example
uses raw nucleotide strings generate during the creation of our
*volpertinger* samples. This serves well as an example how nucleotide
strings are processed line by line and mutated. You can see the
mutations in this example showing up as lower-case letters. 

```{.sh}
scripts/multiple_mutate\
    data/mut-tables/mut.tab \
    < data/2/volpertinger.nucl \
    | head
```

An already existent FASTQ file can be mutated using the `filter_fastq`
tool in cooperation with `multiple-mutate`. The tool `filter_fastq`
enables you to apply transformations to existing fastq files. To this
effect, `filter_fastq` extracts one part out of a FASTQ file (read
name, nucleotide string or quality string) and feeds it into another
sub-program specified between two `@`-signs. The sub-program is required to
take lines of text as input and return the same number of lines on
standard output. The output of the sub-program is then placed into the output
fastq file. By combining `filter_fastq` and `multiple_mutate`, the tool
which applies mutations to strings of nucleotides, a FASTQ file can be
mutated:

```{.sh}
scripts/filter_fastq --nucleotide \
  @ scripts/multiple_mutate data/mut-tables/mut.tab @ \
  < data/2/volpertinger.fastq \
  > data/3/volpertinger_mut.fastq
```
Note how the nucleotide strings of the output FASTQ file now carry
mutations (lower-case letters):

```{.sh}
head data/3/volpertinger_mut.fastq
```

The `filter_fastq.py` script enables you to apply an arbitrary script
or program on just one part of a FASTQ file (ID line, nucleotide line,
quality line). The used script must accept the respective part on
standard input and print the modified version on standard output. The
modified FASTQ file is assembled by `filter_fastq.py` from the output
of its children scripts and printed on standard output. 

On the `filter_fastq.py` call, the @ sign serves as a sentinel
character, which determines start and end of the sub-program's
command line. It can also be any arbitrary other character, as long as
it doesn't occur inside the child script's command line but only at
the beginning and the end.


Combining endogenous and non-endogenous reads
---------------------------------------------

In this example, the endogenous reads from *volptertinger* undergo 
simulated mutation and damage prior to mapping, while the contaminant
reads from *R. etli* do not.

Therefore only now, after applying mutations to our *volpertinger*
reads, is the time to combine the mutated sample reads
and the contaminant reads generated in the last section to one
file. For this purpose, the UNIX tool `cat` is used:

```{.sh}
cat data/3/volpertinger_mut.fastq data/2/retli.fastq \
    > data/3/all.fastq
```

Obtaining mutation rates from mapDamage
---------------------------------------

Damage patterns from mapDamage can be converted into a table
with mutation parameters by least-squares fitting. For this purpose,
the mapDamage output files ending with `... _freq.txt` are needed.

The following command fits a geometric distribution to mapDamage data, 
shows the derived parameters and plots the data with the fitted curve.

The `cut` command is used only to limit the output to a
width acceptable for this manual. Use the bash `>` redirection
operator to write this output into a file suitable for
`multiple_mutate.py`.

```{.sh}
scripts/mapdamage2geomparam \
    --fit-plots data/3/fit_ \
    data/mapdamage/*.txt | \
    cut -f1-6 | \
    column -t
```

The generated plots can be viewed 
<a href="data/3/fit_001_GS136_5pCtoT_freq.txt.pdf">here (C→T)</a> and
<a href="data/3/fit_000_GS136_3pGtoA_freq.txt.pdf">here (G→A)</a>.
fit_000_GS136_3pGtoA_freq.txt.pdf

Generating multiple damage patterns using a parameter table
-----------------------------------------------------------

Sometimes, multiple damage patterns need to be compared. There is a
possibility to generate the `multiple_mutate` input files from one table
which lists all different values of the different mutation parameters. 

This approach will be seen again later, where there is a possibility
to generate many short read mapper calls from exactly the same kind of
parameter table. You can therefore generate appropriate input files
as well as appropriate short read mapper calls out of only one table
which lists all the parameters.

The `fill_template.py` script expects a table, where each row is used
to fill a prespecified template with values. 

For example, if a template is written which looks like this:

```{.sh}
cp data/mut-tmpl/mut-tmpl data/3
column -t data/3/mut-tmpl
```

And a table is created which looks like this:

```{.sh}
cp data/mut-tmpl/tab data/3/mut-tab
column -t data/3/mut-tab
```

several files can be generated with 

```{.sh}
scripts/fill_template \
    data/3/mut-tmpl \
    < data/3/mut-tab
```

Use the `--output` switch of this script to write each file in a
separate file. The argument of `--output` can (and should!) contain
column names of the table, enclosed in braces {...}. This creates a
separate filename per input row. 

We will now write each of the tables shown above to its own file. We
want to name the files using a counting number, but our input table
doesn't yet contain a column with that counter. Therefore we must
first add one.

The mentioned-above table can be prepended with an index
column:

```{.sh}
scripts/index_column --inplace data/3/mut-tab

head data/3/mut-tab | column -t
```

Now, each output of `fill_template.py` can be written to its own 
output file, using the information from the newly-generated `index`
column:

```{.sh}
scripts/fill_template \
    --output "data/3/{index}_filled" \
    data/3/mut-tmpl \
    < data/3/mut-tab

# Show all the generated files
for f in data/3/*_filled; do
    echo " === $f === "
    column -t $f
done
```

If several combinations of mutation parameters shall be tested,
`cross_tab.py` can be used to generate the table from predefined
parameter values, like described with mapper parameters in the next
section.



Generate mapper calls
=====================

To generate the calls to the mapper using different combinations of 
parameters, several files holding the values of the different parameters
are first combined to a table holding all possible combinations of them.

Subsequently, every line is given a unique index which can be referred to
e.g. when writing output files of the mapping process. By this index, each
run writes to a different output file.

The parameter values are saved in several files, one per parameter. 
In this example, the BWA parameters n and k are varied which results
in two files:

```{.sh}
ls data/mapping/*.par
```

The files can have arbitrary filenames, they are in a tabular format
where the column names relate to variables which are set automatically
later in the process.

```{.sh}
column -t data/mapping/n.par
```

To generate all combinations of parameters, two scripts are used:

  * `scripts/cross_tab` expects multiple files and outputs all
    possible combinations of their lines. 

  * `scripts/index_column` This script prepends a counting number 
    to each input line. It can be used to generate index columns for text 
    tables.

Generate all possible combinations of parameters, retaining 1 header
line:

```{.sh}
scripts/cross_tab --head 1 data/mapping/*.par > data/4/partab
head data/4/partab | column -t
```

Add an index column called runidx:
```{.sh}
scripts/index_column --colname runidx --inplace data/4/partab
head data/4/partab | column -t
```

Read now the script `data/mapping/map-bwa.sh` and see how the variables
used there correspond to the column names of partab. The script is
shown in the next code block. 

This is a script which can be called using different parameter
combinations: It calls the mapper `bwa` and forwards the values of the
variables set via `data/4/partab` as command line arguments to the
mapper. 

```{.bash}
#!/bin/bash

## This script performs a mapping using BWA.
## It requires the variables k, n, runidx and fastq be set 
## prior to its execution.

# Fail if any needed variable is not set
set -ue

bwa aln -n ${n} -k ${k}      \
    data/genome/volpertinger \
    data/3/all.fastq         \
    > data/4/${runidx}.sai   \
    2> data/4/${runidx}.log   &&

bwa samse                      \
      data/genome/volpertinger \
      data/4/${runidx}.sai     \
      data/3/all.fastq         \
      > data/4/${runidx}.sam   \
      2>> data/4/${runidx}.log

```

Below the calls are generated.

```{.sh}
# Convert the table into calls that can be executed in the next section
scripts/table2calls  data/4/partab \
                    data/mapping/map-bwa.sh \
                  > data/4/calls
cat data/4/calls
```

Executing multiple mapping runs in parallel
===========================================

For this task, many programs can be used, from simple shell background
spawning using & (in bash) to job managers orchestrating a big network
of worker machines. In this package, a simple program is implemented
which executes a user-definable number of jobs in parallel and
waits with spawning new ones until another of its already started jobs
finishes.

Note that some mappers can use more than one processor core
themselves. Therefore if you spawn multiple mapper processes where
each mapper process utilizes multiple cores, the total number of
utilized cores is the number of cores used per mapper multiplied with
the number of mapper processes launched in parallel.

Invoke `scripts/mcall --help` to get more information about
this tool.

Example: Run the previously generated mapper calls. 

```{.sh}
    # Execute calls, at 2 cores
    scripts/mcall -c data/4/calls -t 2 \
                  --status
    # Standard error was piped to log files, 
    # Standard output was piped to sam files, as specified in the
    # `tmpl` file.
    head data/4/0.log
    head -n15 data/4/0.sam
```



Parsing of SAM files
====================

With the following tools, SAM files can be parsed to gain information of read
names, where they were mapped, which quality score the mapping was assigned
and so on. 

The procedures in this chapter may vary more than the previous ones, 
depending on the research question.

In the setting this package was originally designed for, the names of the
reads carry the information where the reads actually belong to. This
information can subsequently be compared to the actual mapping information
obtained from the SAM file.

Extraction of information
-------------------------

For this purpose, the `sam-extract.R` tool can be used. This tool
converts a SAM file into a table, where the columns can be
informations obtained from the read names or SAM fields. The names of
the SAM fields can be looked up in the SAM specification online, but
the most important ones are:

* qname: read name
* rname: FASTA record name of genome this read was mapped to. `*` if 
         not mapped.
* pos:   base index of mapping position (1-based index!)
* mapq:  quality score assigned by the mapper
* cigar: CIGAR String: Information about gaps and mismatches in
         the alignment read -- reference

Take care not to put any spaces in the argument of --sam-fields.

```{.sh}
scripts/sam_extract --sam-fields qname,rname,pos,mapq \
    data/4/1.sam  >  data/5/1.tab
```

```{.sh}
head data/5/1.tab | column -t
```

```{.sh}
tail data/5/1.tab | column -t
```

Gather all information needed to determine correct mapping
----------------------------------------------------------

The script `add_mapped_organisms` adds two columns to the result
above: The column `true_organism` lists the organism a read stems
from. This is an important information if the FASTA record names of
several species used in the analysis overlap. This is sometimes the
case when multiple eukaryotes are used. The second column,
`mapped_organism`, infers the organism a read has been assigned to by
the mapper. This is done by looking up the FASTA record name, which
the read was mapped to, in all FASTA record names of all species which
where used as reference genomes by the mapper. This FASTA record names
must be unique among all organisms used as mapping reference.

In this example, the organism `volpertinger` provides the endogenous
reads as all reads were mapped only to the `volpertinger` genome. The
`retli` reads are therefore exogenous reads.

To assign the correct organism names to the reads, the script must be provided
with

 * The organism names
 * The FASTA index of the organism's genomes
 * The read names derived from the organism
 * Whether the reads are endogenous or exogenous
 * The table with the mapping information from the previous section
   which shall be augmented

The call is shown below. The script `write_later` at the end of the
pipe is to prevent `merge` from overwriting its own input file too
soon, as the output is meant to replace the input file.

```{.sh}
scripts/add_mapped_organisms \
    --endogenous volpertinger \
                 data/genome/volpertinger.fasta.fai \
                 data/2/volpertinger.coord \
    --exogenous  retli \
                 data/retli/retli.fasta.fai \
                 data/2/retli.coord \
    data/5/1.tab \
    | scripts/write_later data/5/1.tab

head data/5/1.tab | column -t
```

To determine whether the reads were mapped correctly, two more pieces
of information are needed besides the true organism: The true FASTA
record (=chromosome) and the true position. These can be looked up in
the files generated during the read sampling process. In this example,
they were named `volpertinger.coord' and `retli.coord`. Because
information for reads from both origins must be looked up, these two
files must first be concatenated, but while printing the header line
only once:

```{.sh}
scripts/cat_tables   data/2/volpertinger.coord \
                     data/2/retli.coord \
                   > data/5/all.coord
head data/5/all.coord | column -t
```

As can be seen in the output above, the needed information is in the
columns named `record` and `start`. The correct values are found by
comparing the `qname` column of `data/5/1.tab` with the `name` 
column of the freshly-generated `data/5/all.tab`. Additionally we
want to rename the columns holding the true chromosome and position
information to make it clear which column holds which information.

This can be done in one go using the `merge` tool, whose invokation is
written below. The used parameters are explained in the following.

We want to make sure no reads get lost by setting the
`--all-a` option. If a read name cannot be found in `all.coord`, the
tool will print `NA` at the respective position (this should not
happen here). `--all-a-cols` indicates that we want to work on with
all columns of `1.tab`. The input file `1.tab` is also the output file
here, therefore the tool `write_later` is used.

```{.sh}
scripts/merge -a data/5/1.tab qname \
              -b data/5/all.coord name record=true_record start=true_pos \
              --all-a-cols \
              --all-a \
              | scripts/write_later data/5/1.tab

head data/5/1.tab | column -t
```

Now all the information is present to determine whether a read has
been mapped correctly. The last step is writing in a new column
whether a read was mapped correctly. This can be archieved using any
means you can imagine, for this example we will use R. The `pocketR`
tool is a thin wrapper which handles reading and writing of data for
us. The input data will be available as a `data.frame` called
`input`, everything written insides the parentheses of the `return(.)`
statement will be printed. 

The following command adds a new column to the input data which
indicates whether a read was mapped correctly:

```{.sh}
scripts/pocketR '
    within(input, { 
        correct =    
            pos == true_pos  &
            rname == true_record &
            mapped_organism == true_organism })
'  data/5/1.tab \
| scripts/write_later data/5/1.tab

 head data/5/1.tab | column -t
```
 


Grouping of reads
-----------------

As next step, the number of reads are counted which belong to 
certain categories. Here, the categories are:
  * Correctly mapped or not
  * Origin organism
  * Organism a read was mapped to

Again, the R language can be used to express our wishes concisely:
Group the reads by all combinations of:

 * `true_organism`
 * `mapped_organism`
 * correctly mapped

... and count the reads belonging to each category:

the `cbind` function is needed in order to rename the column containing 
the read count. `qname` can be substituted here by any valid input
column name, as its only used for counting (each column is equal in
length). 

```{.sh}
scripts/pocketR '
    aggregate( cbind(count=qname) ~ true_organism + mapped_organism + correct,
        FUN=length, data=input)
' data/5/1.tab \
> data/5/1.agg

cat data/5/1.agg | column -t
```

This format may be used to plot the read fate of a single mapper run
and to derive the measures sensitivity and specificity:

```{.sh}
scripts/plot_read_fate    true_organism mapped_organism \
                          correct       count \
                          data/5/1.pdf  data/5/1.agg
```

<a href="data/5/1.pdf">Click here</a> to see the plot.

Sensitivity and specificity
---------------------------

  * **Sensitivity** (recall) shows how many reads have been mapped
    correctly by the mapper which are supposed to map.
  * **Specificity** (precision) shows how many reads have been
    correctly identified as non-endogenous and were therefore not
    mapped.
  * **Balanced control rate*** (BCR) is the mean of sensitivity and
    specificity

If non-endogenous reads were included in the reads, like we did by
including the *R. etli* reads, both measures can be calculated.

The following script needs the same kind of input as the
`plot-read-fate` script. Additionally, a list of organisms must be
specified, whose genomes the mapper used as a reference. 

If you specify multiple organisms, separate them by commas and don't
include any spaces.

```{.sh}
scripts/sensspec --c-morg mapped_organism \
                 --c-torg true_organism \
                 data/5/1.agg volpertinger \
    > data/5/1.parameters

column -t data/5/1.parameters
```



Repeat all steps for every SAM file
-----------------------------------

The code needed to evaluate the data generated by the mapper might as
well be included in the mapping template script introduced in the last
section. If this is done, the data evaluation can be as well
parallelized as the mapping process.

All scripts used here were already introduced in this section.

The files `data/5/all.tab` and `data/5/all.recids` must be calculated
prior to execution of this script. This has been done in this section 
as well.

Browse the <a href="data/5">directory `data/5`</a> to see the results.

```{.sh}
for sam in data/4/*.sam; do
    # Generate output prefix p from input name: `4.sam` -> `4`
    bn=$(basename $sam)
    p=${bn%.sam}

    # Extract SAM fields
    scripts/sam_extract --sam-fields qname,rname,pos,mapq \
        data/4/${p}.sam  >  data/5/${p}.tab

    # Mark correctly/incorrectly mapped reads
    scripts/add_mapped_organisms \
        --endogenous volpertinger \
                     data/genome/volpertinger.fasta.fai \
                     data/2/volpertinger.coord \
        --exogenous  retli \
                     data/retli/retli.fasta.fai \
                     data/2/retli.coord \
        data/5/${p}.tab \
        | scripts/write_later data/5/${p}.tab

    # Determine true origin information for each read
    scripts/merge -a data/5/${p}.tab qname \
                  -b data/5/all.coord name record=true_record start=true_pos \
                  --all-a-cols \
                  --all-a \
                  | scripts/write_later data/5/${p}.tab

    # Determine whether each read was correctly mapped
    scripts/pocketR '
        within(input, { 
            correct =    
                pos == true_pos  &
                rname == true_record &
                mapped_organism == true_organism })
    '  data/5/${p}.tab \
    | scripts/write_later data/5/${p}.tab

    # Count reads per origin/target organism and mapping status
    scripts/pocketR '
        aggregate( cbind(count=qname) ~ true_organism + mapped_organism + correct,
            FUN=length, data=input) ' \
    data/5/${p}.tab \
    > data/5/${p}.agg

    # Plot mapping targets per origin organism 
    scripts/plot_read_fate    true_organism    mapped_organism \
                              correct          count \
                              data/5/${p}.pdf  data/5/${p}.agg

    # Calculate sensitivity, specificity and balanced accuracy
    scripts/sensspec --c-morg mapped_organism \
                     --c-torg true_organism \
                     data/5/${p}.agg volpertinger \
        > data/5/${p}.performance

    echo "$sam done. -> Generated data/5/${p}.{tab,agg,pdf,performance}"

done
```

    

Comparing multiple mapper runs
==============================

The file `data/4/partab` holds the information which parameters were
used for which mapping run. By relating the output measures like
sensitivity, specificity or balanced accuracy to these parameters, the
influence of individual parameters can be assessed.

First step is to combine the output measures of all runs:

To this effect, to each of the `.performance`-files (contain
sensitivity, specificity, bcr) generated in the last section, the run
number is added as a separate column, then the `.performance`-files of
all runs are concatenated to one table.  Because the run number was
added before, the origin of all values is still clear.

```{.sh}
# Add the run number to each .performance file
for f in data/5/*.performance; do
    i=$(basename ${f%.performance})

    scripts/add_const_column "$f" runidx "$i" \
        > "data/6/${i}.performance"
done

# Concatenate all tables, but print the header line only once.
scripts/cat_tables data/6/*.performance \
                 > data/6/performance

cat data/6/performance | column -t
```

Next, the parameter values belonging to the run indices are joined in, 
appending the parameter columns to `data/6/parameters` itself.

```{.sh}
scripts/merge -a data/6/performance runidx \
              -b data/4/partab      runidx \
              --all-a-cols --all-b-cols --all-a \
           | scripts/write_later data/6/performance

head data/6/performance | column -t
```

The value of one parameter can be plotted against some measure. The
following command shows plots where the X and Y axis are free to
choose. If multiple runs yield a similar score on the Y axis, their
data points are merged to form one bigger dot on the plot. Here all
numbers are rounded to one significant digit, because only 8 mapping
runs are compared in this example. Therefore results must be
aggregated coarsely to demonstrate the results.

```{.sh}
# Plot n versus BCR
scripts/plot_parameter_effects --signif 1 data/6/performance n bcr \
    data/6/n.pdf

# Plot k versus BCR
scripts/plot_parameter_effects --signif 1 data/6/performance k bcr \
    data/6/k.pdf
```

View the plots: <a href="data/6/n.pdf">n vs. BCR</a> and
 <a href="data/6/k.pdf">k vs. BCR</a>.

 It can be seen that n seems to have an impact on the BCR whereas k
 does not. The BCR rises and falls again because the gain in
 sensitivity is offset by the loss in specificity if n rises too high.


Overview of the scripts by category
===================================

Below you find a list of the scripts in this package, grouped by rough
application situations. 

More specific help for each command can be obtained by invoking the respective
command with the `--help` or `-h` command line argument.

General file and table manipulation
-----------------------------------

Name                 Purpose
----                 -------
add_const_column     Add a column with a column name and fixed value.
cat_tables           Concatenate tables with the same headers.
cross_tab            Print all combinations of input table rows.
fill_template        Output copies of a file, where placeholders are replaced by values read from a table.
index_column         Add a column with a name and a counting number to the table.
merge                Based on values of table 1, look up corresponding rows of table 2 (table join).
pocketR              Change text tables by R commands.
write_later          Cache the input and write it only after input completion, to enable file modification without (explicit) temporary files.

Read sampling and fitering
--------------------------

Name          Purpose
----          -------
filter_fastq  Apply a program to ID, nucleotide or quality string of each read.
uniform       Sample reads from a reference genome.
synth_fastq   Create a FASTQ file from input IDs, nucleotide strings and quality strings.

Introducing mutations into reads
--------------------------------

Name                 Purpose
----                 -------
geom_induce          Change bases of input nucleotide strings with a probability dependent of proximity to the string beginning or end.
mapdamage2geomparam  Parse output of mapDamage an output a table of base mutation probabilities.
multiple_mutate      Based on a table of base mutation probabilities, apply multiple mutation rounds to nucleotide strings.
plot_mutation_probabilities  Plot mutation probability versus base position.

Parallel program calls (e.g. mappers)
--------------------------------------

Name             Purpose
----             -------
mcall            Read a list of program calls and invoke several of them in parallel.
table2calls      Based on a table of parameters, generate calls to a script with changing parameters.


SAM parsing and result data handling
------------------------------------

Name                     Purpose
----                     -------
add_mapped_organisms     Add organisms relating to FASTA record names in the input tables to the output.
plot_read_fate           Plot how many reads from which origin were mapped to which target organism correctly or incorrectly.
plot_parameter_effects   Plot how much mapping parameters influence sensitivity or specificity
sam_extract              Extract information from a SAM file into a text table.
sensspec                 Calculate sensitivity and specificity of a mapping run.



Glossary
========

Some terms are explained here, which are used in the rest of the
manual and may have a special meaning:

~table: A text table is the most widely used data format in this
project. It is a simple text file, where the columns are separated by
a special character. Usually this is a tabulator (tab) character, but
sometimes space-separated tables are also used. 

~FASTA record: A FASTA record is a block in a fasta file which is
delimited by two > signs. It consists of the following parts:
 
 1)  The header line: >.................(line end). 
    a) The FASTA identifier, which is the first 'word' of the header
       line. E.g. in a header line 
       
    >A1 Felis catus genome ID:012345...
        
        the FASTA identifier is 'A1'.
    b) The FASTA record description: All the text between identifier
       and line end. 
 2) Biological sequence data, IUPAC one-character-code. Should not be wider 
    than 80 characters and may contain whitespace.
    E.g. nucleotides: ACCTCTCTACCT...

~FASTA identifier: -> FASTA record

~FASTA description: -> FASTA record

~file offset: This is the
distance in bytes from the beginning of a file. The first character of
a file has the offset 0.

~offset: -> file offset

~i-based index: An index is i-based if it starts counting with
number i. For example, the mapping position (pos) field of a SAM file 
is a 1-based index. If the position 1 is written there, the read maps
to the first base of the genome. Conversely, if the index were
0-based,
the first base of the genome would be referenced with the number 0.

~0-based: -> i-based index

~1-based: -> i-based index

~standard input: Many commands expect input on this stream. Input can
be provided either by typing into the console, by using the < operator
to provide input from a file or by the | operator, which forwards the
content on standard output of a previous command to standard input of
this command. Refer to the "Redirection" section of your shell for
more information.

~standard output: All output a command writes is by default redirected
to the standard output or standard error streams. Per convention,
standard output is used for the results of the program, whereas
standard error is used for status and error messages. Content on
standard output can be written in a file by using the > operator or
redirected to standard input of another command by using the |
operator. Standard error output can be redirected to a file using the
2> operator. Consult the "Redirection" section of your shell for more
information. 

~standard error: -> standard output

~command line argument: a value which is written on the command line
behind the name of the program which shall be invoked. They are
forwarded to the program and influence it. The valid parameters of a
program are described in the program's manual. (often accessible via
the `--help` command line argument. 

~command line switch: -> command line argument



